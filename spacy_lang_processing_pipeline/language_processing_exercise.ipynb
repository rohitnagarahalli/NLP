{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1c5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ed3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain   |      |   \n",
      "america   |      |   \n",
      "ate   |      |   \n",
      "100   |      |   \n",
      "$   |      |   \n",
      "of   |      |   \n",
      "samosa   |      |   \n",
      ".   |      |   \n",
      "Then   |      |   \n",
      "he   |      |   \n",
      "said   |      |   \n",
      "I   |      |   \n",
      "can   |      |   \n",
      "do   |      |   \n",
      "this   |      |   \n",
      "all   |      |   \n",
      "day   |      |   \n",
      ".   |      |   \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,  '  |  ', token.pos_, '  |  ', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc161f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf76d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2ebc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce2df8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1abdc982c20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1abdc982740>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1abdca58740>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1abdcb33c40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1abdcb1ed40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1abdd4e36f0>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85178587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain   |   PROPN   |   Captain\n",
      "america   |   PROPN   |   america\n",
      "ate   |   VERB   |   eat\n",
      "100   |   NUM   |   100\n",
      "$   |   NUM   |   $\n",
      "of   |   ADP   |   of\n",
      "samosa   |   PROPN   |   samosa\n",
      ".   |   PUNCT   |   .\n",
      "Then   |   ADV   |   then\n",
      "he   |   PRON   |   he\n",
      "said   |   VERB   |   say\n",
      "I   |   PRON   |   I\n",
      "can   |   AUX   |   can\n",
      "do   |   VERB   |   do\n",
      "this   |   PRON   |   this\n",
      "all   |   DET   |   all\n",
      "day   |   NOUN   |   day\n",
      ".   |   PUNCT   |   .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, '  |  ', token.pos_, '  |  ', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe10c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc   |   ORG   |   Companies, agencies, institutions, etc.\n",
      "$45 billion   |   MONEY   |   Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '  |  ',  ent.label_, '  |  ', spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe28459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e79ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('ner', source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b2754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc   |   ORG   |   Companies, agencies, institutions, etc.\n",
      "$45 billion   |   MONEY   |   Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '  |  ',  ent.label_, '  |  ', spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e34d93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959bf57",
   "metadata": {},
   "source": [
    "Excersie: 1\n",
    "- Get all the proper nouns from a given text in a list and also count how many of them.\n",
    "- Proper Noun means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c00f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Nouns: [Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n",
      "Count: 7\n"
     ]
    }
   ],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''\n",
    "\n",
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "#creating the nlp object\n",
    "doc = nlp(text)\n",
    "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
    "count = len(proper_nouns)\n",
    "print(\"Proper Nouns:\", proper_nouns)\n",
    "print(\"Count:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1e6f47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
       "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
       "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_0 = doc[0]\n",
    "token_0 = token_0.text.title()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb20ee5",
   "metadata": {},
   "source": [
    "Excersie: 2\n",
    "- Get all companies names from a given text and also the count of them.\n",
    "- Hint: Use the spacy ner functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f7876f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Names: ['Raju', 'Paris', 'London', 'Dubai', 'Rome', 'Mohan', 'Hyderabad']\n",
      "Count: 10\n"
     ]
    }
   ],
   "source": [
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "comapany = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "count = len(comapany)\n",
    "print(\"Company Names:\", proper_nouns)\n",
    "print(\"Count:\", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
